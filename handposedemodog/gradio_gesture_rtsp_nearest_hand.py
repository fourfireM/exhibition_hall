import cv2
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
import numpy as np
import time
import gradio as gr
import threading
from PIL import Image
import asyncio
import concurrent.futures
from move import (
    move_forward, move_backward, turn_left, turn_right,
    strafe_left, strafe_right, stop_movement
)
import os 
import requests

class GradioGestureRecognizer:
    def __init__(self):
        # é…ç½®æ¨¡å‹è·¯å¾„
        self.model_path = '/home/myb/handposedemodog/gesture_recognizer.task'
        
        # åˆå§‹åŒ–å˜é‡
        self.latest_result = None
        self.latest_timestamp = 0
        self.is_running = False
        self.cap = None
        self.recognizer = None
        self.camera_thread = None
        self.current_frame = None
        self.current_gesture = "æ— æ‰‹åŠ¿"
        self.current_handedness = "æœªæ£€æµ‹åˆ°æ‰‹"
        
        # æ€§èƒ½ä¼˜åŒ–ç›¸å…³
        self.frame_skip_counter = 0
        self.frame_skip_interval = 1  # æ¯1å¸§å¤„ç†ä¸€æ¬¡æ‰‹åŠ¿è¯†åˆ«ï¼Œæé«˜å“åº”é€Ÿåº¦
        self.last_process_time = time.time()
        
        # æœºå™¨ç‹—æ§åˆ¶ç›¸å…³
        self.robot_control_enabled = False
        self.current_robot_action = "åœæ­¢"
        self.last_gesture_time = 0
        self.gesture_cooldown = 0.08  # æ‰‹åŠ¿è¯†åˆ«å†·å´æ—¶é—´ï¼ˆç§’ï¼‰è¿›ä¸€æ­¥å‡å°‘
        self.min_confidence = 0.5  # æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼
        self.last_action_name = None  # è®°å½•ä¸Šä¸€ä¸ªåŠ¨ä½œï¼Œç”¨äºå¿«é€Ÿåˆ‡æ¢æ£€æµ‹
        
        # å¼‚æ­¥æ‰§è¡Œç›¸å…³
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)
        self.pending_actions = set()  # è·Ÿè¸ªæ­£åœ¨æ‰§è¡Œçš„åŠ¨ä½œ
        
        # RTSPæµç›¸å…³
        rtsp_data = requests.get("http://localhost:18080/signalservice/video/open")
        if os.environ.get("VIDEO_SOURCE") == "unitree":
            self.rtsp_url = rtsp_data.json()["data"]["unitree_rtsp_url"]
        elif os.environ.get("VIDEO_SOURCE") == "realsense":
            self.rtsp_url = rtsp_data.json()["data"]["realsense_rtsp_url"]
        elif os.environ.get("VIDEO_SOURCE") == "orbbec":
            self.rtsp_url = rtsp_data.json()["data"]["orbbec_rtsp_url"]
        else:
            self.rtsp_url = rtsp_data.json()["data"]["lite3_rtsp_url"]
        self.rtsp_reconnect_attempts = 0
        self.max_reconnect_attempts = 3
        
        # åˆ›å»ºæ‰‹åŠ¿è¯†åˆ«å™¨é€‰é¡¹
        base_options = python.BaseOptions(model_asset_path=self.model_path)
        self.options = vision.GestureRecognizerOptions(
            base_options=base_options,
            running_mode=vision.RunningMode.LIVE_STREAM,
            result_callback=self.process_result,
            num_hands=5  # è¯†åˆ«æœ€å¤š5åªæ‰‹
        )
    
    def calculate_hand_size(self, hand_landmarks):
        """è®¡ç®—æ‰‹éƒ¨å¤§å°ï¼ˆåŸºäºå…³é”®ç‚¹çš„è¾¹ç•Œæ¡†é¢ç§¯ï¼‰"""
        if not hand_landmarks:
            return 0
        
        # è·å–æ‰€æœ‰å…³é”®ç‚¹çš„xå’Œyåæ ‡
        x_coords = [landmark.x for landmark in hand_landmarks]
        y_coords = [landmark.y for landmark in hand_landmarks]
        
        # è®¡ç®—è¾¹ç•Œæ¡†
        min_x, max_x = min(x_coords), max(x_coords)
        min_y, max_y = min(y_coords), max(y_coords)
        
        # è®¡ç®—é¢ç§¯ï¼ˆå½’ä¸€åŒ–åæ ‡ç³»ä¸­çš„é¢ç§¯ï¼‰
        area = (max_x - min_x) * (max_y - min_y)
        return area
    
    def find_largest_hand(self, result):
        """ä»è¯†åˆ«ç»“æœä¸­æ‰¾åˆ°æœ€å¤§çš„æ‰‹ï¼ˆæœ€é è¿‘æ‘„åƒå¤´çš„æ‰‹ï¼‰"""
        if not result.hand_landmarks:
            return None, None, None, None
        
        largest_hand_idx = 0
        largest_size = 0
        
        # éå†æ‰€æœ‰æ£€æµ‹åˆ°çš„æ‰‹ï¼Œæ‰¾åˆ°æœ€å¤§çš„ä¸€åª
        for i, hand_landmarks in enumerate(result.hand_landmarks):
            hand_size = self.calculate_hand_size(hand_landmarks)
            if hand_size > largest_size:
                largest_size = hand_size
                largest_hand_idx = i
        
        # è·å–æœ€å¤§æ‰‹çš„ä¿¡æ¯
        gesture_name = None
        confidence = 0.0
        hand_label = None
        
        # è·å–æ‰‹åŠ¿ä¿¡æ¯
        if result.gestures and len(result.gestures) > largest_hand_idx and result.gestures[largest_hand_idx]:
            gesture_name = result.gestures[largest_hand_idx][0].category_name
            confidence = result.gestures[largest_hand_idx][0].score
        
        # è·å–æ‰‹æ€§ä¿¡æ¯
        if result.handedness and len(result.handedness) > largest_hand_idx and result.handedness[largest_hand_idx]:
            hand_label = result.handedness[largest_hand_idx][0].category_name
            # ç¿»è½¬æ‰‹æ€§æ ‡ç­¾ï¼ˆå› ä¸ºå›¾åƒæ˜¯ç¿»è½¬çš„ï¼‰
            if hand_label == "Left":
                hand_label = "Right"  # ç¿»è½¬åçš„å·¦æ‰‹å®é™…æ˜¯å³æ‰‹
            elif hand_label == "Right": 
                hand_label = "Left"   # ç¿»è½¬åçš„å³æ‰‹å®é™…æ˜¯å·¦æ‰‹
        
        return gesture_name, confidence, hand_label, largest_hand_idx
    
    def process_result(self, result, output_image, timestamp_ms):
        """å¤„ç†æ‰‹åŠ¿è¯†åˆ«ç»“æœçš„å›è°ƒå‡½æ•°"""
        self.latest_result = result
        self.latest_timestamp = timestamp_ms
        
        # æ‰¾åˆ°æœ€å¤§çš„æ‰‹
        gesture_name, confidence, hand_label, largest_hand_idx = self.find_largest_hand(result)
        
        # æ›´æ–°æ‰‹åŠ¿ä¿¡æ¯
        if gesture_name and confidence > 0:
            self.current_gesture = f"{gesture_name} ({confidence:.2f})"
        else:
            self.current_gesture = "æ— æ‰‹åŠ¿"
        
        # æ›´æ–°æ‰‹æ€§ä¿¡æ¯
        if hand_label:
            hand_score = result.handedness[largest_hand_idx][0].score if result.handedness and len(result.handedness) > largest_hand_idx else 0.0
            self.current_handedness = f"{hand_label} ({hand_score:.2f})"
        else:
            self.current_handedness = "æœªæ£€æµ‹åˆ°æ‰‹"
        
        # æœºå™¨ç‹—æ§åˆ¶é€»è¾‘ï¼ˆåªä½¿ç”¨æœ€å¤§çš„æ‰‹ï¼‰
        if gesture_name and confidence > 0:
            robot_action = self.map_gesture_to_robot_action(gesture_name, hand_label, confidence)
            if robot_action:
                action_name, action_func = robot_action
                action_result = self.execute_robot_action(action_name, action_func)
                print(f"æœºå™¨ç‹—æ§åˆ¶: {action_result}")
        # æ²¡æœ‰æ£€æµ‹åˆ°æ‰‹åŠ¿æ—¶ä¸éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œå› ä¸ºå·²ç»ç§»é™¤äº†ç¨³å®šæ€§æ£€æŸ¥
    
    def map_gesture_to_robot_action(self, gesture_name, hand_label, confidence):
        """
        å°†æ‰‹åŠ¿æ˜ å°„åˆ°æœºå™¨ç‹—åŠ¨ä½œ
        
        æ˜ å°„è§„åˆ™ï¼š
        ğŸ‘† Pointing_Upï¼ˆå•æ‰‹å‘ä¸ŠæŒ‡ï¼‰    â†’ å‰è¿›
        ğŸ‘‡ Thumb_Downï¼ˆæ‹‡æŒ‡å‘ä¸‹ï¼‰      â†’ åé€€  
        âœ‹ Open_Palmï¼ˆå·¦æ‰‹å¼ å¼€ï¼‰       â†’ å·¦ç§»
        âœ‹ Open_Palmï¼ˆå³æ‰‹å¼ å¼€ï¼‰       â†’ å³ç§»
        ğŸ‘Š Closed_Fistï¼ˆå·¦æ‰‹æ¡æ‹³ï¼‰     â†’ å·¦è½¬
        ğŸ‘Š Closed_Fistï¼ˆå³æ‰‹æ¡æ‹³ï¼‰     â†’ å³è½¬
        ğŸ›‘ Victoryï¼ˆVæ‰‹åŠ¿ï¼‰           â†’ åœæ­¢
        """
        if not self.robot_control_enabled or confidence < self.min_confidence:
            return None
        
        current_time = time.time()
        
        # æ£€æŸ¥å†·å´æ—¶é—´
        if current_time - self.last_gesture_time < self.gesture_cooldown:
            return None
        
        # æ‰‹åŠ¿åˆ°åŠ¨ä½œçš„æ˜ å°„
        action_map = {
            ("Pointing_Up", None): ("å‰è¿›", move_forward),
            ("Thumb_Down", None): ("åé€€", move_backward),
            # ("Open_Palm", "Left"): ("å·¦ç§»", strafe_left),
            # ("Open_Palm", "Right"): ("å³ç§»", strafe_right),
            # ("Closed_Fist", "Left"): ("å·¦è½¬", turn_left),
            # ("Closed_Fist", "Right"): ("å³è½¬", turn_right),
            ("Open_Palm", "Left"): ("å·¦ç§»", strafe_right),
            ("Open_Palm", "Right"): ("å³ç§»", strafe_left),
            ("Closed_Fist", "Left"): ("å·¦è½¬", turn_right),
            ("Closed_Fist", "Right"): ("å³è½¬", turn_left),
            ("Victory", None): ("åœæ­¢", stop_movement),
        }
        
        # æŸ¥æ‰¾åŒ¹é…çš„åŠ¨ä½œ
        for (gesture, hand), (action_name, action_func) in action_map.items():
            if gesture_name == gesture and (hand is None or hand_label == hand):
                return action_name, action_func
        
        return None
    
    def execute_robot_action_async(self, action_name, action_func):
        """å¼‚æ­¥æ‰§è¡Œæœºå™¨ç‹—åŠ¨ä½œçš„åŒ…è£…å‡½æ•°"""
        try:
            print(f"å¼€å§‹å¼‚æ­¥æ‰§è¡Œ: {action_name}")
            result = action_func()
            print(f"å¼‚æ­¥æ‰§è¡Œå®Œæˆ: {action_name}")
            # ä»å¾…æ‰§è¡Œåˆ—è¡¨ä¸­ç§»é™¤
            self.pending_actions.discard(action_name)
            return result
        except Exception as e:
            print(f"å¼‚æ­¥æ‰§è¡Œå¤±è´¥: {action_name}, é”™è¯¯: {str(e)}")
            self.pending_actions.discard(action_name)
            return f"æ‰§è¡Œå¤±è´¥: {str(e)}"

    def execute_robot_action(self, action_name, action_func):
        """æ‰§è¡Œæœºå™¨ç‹—åŠ¨ä½œ - å¼‚æ­¥éé˜»å¡ç‰ˆæœ¬"""
        try:
            current_time = time.time()
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒåŠ¨ä½œåœ¨æ‰§è¡Œä¸­
            if action_name in self.pending_actions:
                return f"æ‰§è¡Œä¸­: {action_name}"
            
            # åªè¿›è¡ŒåŸºæœ¬çš„å†·å´æ£€æŸ¥ï¼Œé¿å…è¿‡äºé¢‘ç¹çš„ç›¸åŒåŠ¨ä½œ
            time_since_last = current_time - self.last_gesture_time
            
            # å¦‚æœæ˜¯ä¸åŒçš„åŠ¨ä½œï¼Œç«‹å³æ‰§è¡Œ
            # å¦‚æœæ˜¯ç›¸åŒçš„åŠ¨ä½œï¼Œæ£€æŸ¥å†·å´æ—¶é—´
            if self.last_action_name != action_name or time_since_last >= self.gesture_cooldown:
                # å¼‚æ­¥æ‰§è¡ŒåŠ¨ä½œï¼Œä¸é˜»å¡ä¸»çº¿ç¨‹
                self.pending_actions.add(action_name)
                future = self.executor.submit(self.execute_robot_action_async, action_name, action_func)
                
                # æ›´æ–°çŠ¶æ€ï¼ˆç«‹å³æ›´æ–°ï¼Œä¸ç­‰å¾…æ‰§è¡Œå®Œæˆï¼‰
                self.current_robot_action = action_name
                self.last_gesture_time = current_time
                self.last_action_name = action_name
                
                print(f"æäº¤æœºå™¨ç‹—åŠ¨ä½œ: {action_name} (å»¶è¿Ÿ: {time_since_last:.3f}s)")
                return f"æ‰§è¡Œ: {action_name}"
            else:
                # ç›¸åŒåŠ¨ä½œåœ¨å†·å´æœŸå†…
                return f"å†·å´ä¸­: {action_name} ({self.gesture_cooldown - time_since_last:.2f}s)"
            
        except Exception as e:
            print(f"æäº¤æœºå™¨ç‹—åŠ¨ä½œå¤±è´¥: {str(e)}")
            return f"æäº¤å¤±è´¥: {str(e)}"
    
    def toggle_robot_control(self):
        """åˆ‡æ¢æœºå™¨ç‹—æ§åˆ¶çŠ¶æ€"""
        self.robot_control_enabled = not self.robot_control_enabled
        if not self.robot_control_enabled:
            # åœç”¨æ§åˆ¶æ—¶è‡ªåŠ¨åœæ­¢æœºå™¨ç‹—
            try:
                stop_movement()
                self.current_robot_action = "åœæ­¢"
            except:
                pass
        return self.robot_control_enabled
    
    def reconnect_rtsp(self):
        """é‡è¿RTSPæµ"""
        if self.rtsp_reconnect_attempts >= self.max_reconnect_attempts:
            print(f"RTSPé‡è¿å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•° {self.max_reconnect_attempts}")
            return False
        
        self.rtsp_reconnect_attempts += 1
        print(f"å°è¯•é‡è¿RTSPæµ... (ç¬¬{self.rtsp_reconnect_attempts}æ¬¡)")
        
        # é‡Šæ”¾å½“å‰è¿æ¥
        if self.cap:
            self.cap.release()
        
        # é‡æ–°è¿æ¥
        time.sleep(1)  # ç­‰å¾…1ç§’å†é‡è¿
        self.cap = cv2.VideoCapture(self.rtsp_url, cv2.CAP_FFMPEG)
        
        if self.cap.isOpened():
            # é‡æ–°è®¾ç½®å‚æ•°
            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
            self.cap.set(cv2.CAP_PROP_FPS, 20)
            self.cap.set(cv2.CAP_PROP_OPEN_TIMEOUT_MSEC, 3000)
            self.cap.set(cv2.CAP_PROP_READ_TIMEOUT_MSEC, 1000)
            
            print("RTSPæµé‡è¿æˆåŠŸ")
            self.rtsp_reconnect_attempts = 0  # é‡ç½®é‡è¿è®¡æ•°
            return True
        else:
            print("RTSPæµé‡è¿å¤±è´¥")
            return False
    
    def find_external_camera(self):
        """æŸ¥æ‰¾å¤–æ¥æ‘„åƒå¤´"""
        available_cameras = []
        
        # æ£€æµ‹å‰10ä¸ªå¯èƒ½çš„æ‘„åƒå¤´ç´¢å¼•
        for i in range(10):
            cap = cv2.VideoCapture(i)
            if cap.isOpened():
                ret, frame = cap.read()
                if ret:
                    available_cameras.append(i)
                cap.release()
        
        if available_cameras:
            # ä¼˜å…ˆé€‰æ‹©é0å·æ‘„åƒå¤´ï¼ˆé€šå¸¸0æ˜¯å†…ç½®æ‘„åƒå¤´ï¼‰
            for cam_id in available_cameras:
                if cam_id != 0:
                    return cam_id
            return available_cameras[0]
        return None
    
    def draw_landmarks_on_image(self, rgb_image, detection_result):
        """åœ¨å›¾åƒä¸Šç»˜åˆ¶æ‰‹éƒ¨å…³é”®ç‚¹"""
        if not detection_result or not detection_result.hand_landmarks:
            return rgb_image
        
        annotated_image = np.copy(rgb_image)
        height, width, _ = annotated_image.shape
        
        # æ‰¾åˆ°æœ€å¤§çš„æ‰‹
        _, _, _, largest_hand_idx = self.find_largest_hand(detection_result)
        
        # ä¸ºæ‰€æœ‰æ£€æµ‹åˆ°çš„æ‰‹ç»˜åˆ¶å…³é”®ç‚¹
        for hand_idx, hand_landmarks in enumerate(detection_result.hand_landmarks):
            # å°†å…³é”®ç‚¹è½¬æ¢ä¸ºåƒç´ åæ ‡
            landmark_points = []
            
            for landmark in hand_landmarks:
                x = int(landmark.x * width)
                y = int(landmark.y * height)
                landmark_points.append((x, y))
            
            # é€‰æ‹©é¢œè‰²ï¼šæœ€å¤§çš„æ‰‹ç”¨çº¢è‰²çªå‡ºæ˜¾ç¤ºï¼Œå…¶ä»–æ‰‹ç”¨ç»¿è‰²
            if hand_idx == largest_hand_idx:
                point_color = (255, 0, 0)  # çº¢è‰²å…³é”®ç‚¹
                line_color = (255, 0, 0)   # çº¢è‰²è¿æ¥çº¿
                point_radius = 5           # æ›´å¤§çš„å…³é”®ç‚¹
                line_thickness = 3         # æ›´ç²—çš„è¿æ¥çº¿
            else:
                point_color = (0, 255, 0)  # ç»¿è‰²å…³é”®ç‚¹
                line_color = (0, 255, 0)   # ç»¿è‰²è¿æ¥çº¿
                point_radius = 3           # æ™®é€šå…³é”®ç‚¹
                line_thickness = 2         # æ™®é€šè¿æ¥çº¿
            
            # ç»˜åˆ¶å…³é”®ç‚¹
            for point in landmark_points:
                cv2.circle(annotated_image, point, point_radius, point_color, -1)
            
            # ç»˜åˆ¶è¿æ¥çº¿ï¼ˆæ‰‹éƒ¨éª¨æ¶ï¼‰
            connections = [
                (0, 1), (1, 2), (2, 3), (3, 4),  # æ‹‡æŒ‡
                (0, 5), (5, 6), (6, 7), (7, 8),  # é£ŸæŒ‡
                (5, 9), (9, 10), (10, 11), (11, 12),  # ä¸­æŒ‡
                (9, 13), (13, 14), (14, 15), (15, 16),  # æ— åæŒ‡
                (13, 17), (17, 18), (18, 19), (19, 20),  # å°æŒ‡
                (0, 17)  # æ‰‹æŒ
            ]
            
            for connection in connections:
                if connection[0] < len(landmark_points) and connection[1] < len(landmark_points):
                    cv2.line(annotated_image, landmark_points[connection[0]], 
                            landmark_points[connection[1]], line_color, line_thickness)
            
            # ä¸ºæœ€å¤§çš„æ‰‹æ·»åŠ æ–‡æœ¬æ ‡è¯†
            if hand_idx == largest_hand_idx and landmark_points:
                # åœ¨æ‰‹è…•ä½ç½®æ·»åŠ "ä¸»æ§æ‰‹"æ ‡è¯†
                wrist_point = landmark_points[0]
                cv2.putText(annotated_image, "Main Hand", 
                           (wrist_point[0] - 30, wrist_point[1] - 20), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
        
        return annotated_image
    
    def camera_loop(self):
        """æ‘„åƒå¤´å¾ªç¯å¤„ç† - RTSPä¼˜åŒ–ç‰ˆ"""
        frame_count = 0
        last_fps_time = time.time()
        
        while self.is_running and self.cap is not None:
            # æ¸…ç†ç¼“å†²åŒºï¼Œè·å–æœ€æ–°å¸§ï¼ˆå‡å°‘å»¶è¿Ÿçš„å…³é”®ï¼‰
            for _ in range(self.cap.get(cv2.CAP_PROP_BUFFERSIZE) or 1):
                ret, frame = self.cap.read()
                if not ret:
                    break
            
            if not ret:
                print("RTSPæµè¯»å–å¤±è´¥ï¼Œå°è¯•é‡è¿...")
                if self.reconnect_rtsp():
                    continue  # é‡è¿æˆåŠŸï¼Œç»§ç»­å¾ªç¯
                else:
                    break     # é‡è¿å¤±è´¥ï¼Œé€€å‡ºå¾ªç¯
            
            frame_count += 1
            
            # ç¿»è½¬å›¾åƒï¼Œä½¿å…¶åƒé•œå­ä¸€æ ·
            frame = cv2.flip(frame, 1)
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # å¸§è·³è¿‡ä¼˜åŒ–ï¼šä¸æ˜¯æ¯å¸§éƒ½è¿›è¡Œæ‰‹åŠ¿è¯†åˆ«
            self.frame_skip_counter += 1
            should_process = self.frame_skip_counter >= self.frame_skip_interval
            
            if should_process and self.recognizer:
                # åˆ›å»ºMediaPipe Imageå¯¹è±¡
                mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)
                
                # è®¡ç®—æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
                timestamp_ms = int(time.time() * 1000)
                
                # å¼‚æ­¥è¯†åˆ«æ‰‹åŠ¿
                self.recognizer.recognize_async(mp_image, timestamp_ms)
                self.frame_skip_counter = 0
            
            # ç»˜åˆ¶æ‰‹éƒ¨å…³é”®ç‚¹ï¼ˆæ¯å¸§éƒ½ç»˜åˆ¶ï¼Œä¿æŒè§†è§‰è¿ç»­æ€§ï¼‰
            if self.latest_result:
                annotated_frame = self.draw_landmarks_on_image(rgb_frame, self.latest_result)
                self.current_frame = annotated_frame
            else:
                self.current_frame = rgb_frame
            
            # åŠ¨æ€FPSç›‘æ§
            if frame_count % 30 == 0:
                current_time = time.time()
                fps = 30 / (current_time - last_fps_time)
                print(f"RTSPæµFPS: {fps:.1f}")
                last_fps_time = current_time
            
            # å‡å°‘ç¡çœ æ—¶é—´ï¼Œæé«˜å“åº”é€Ÿåº¦
            time.sleep(0.01)  # 100fpså¤„ç†é€Ÿåº¦ï¼Œä½†å®é™…å—RTSPæµé™åˆ¶
    
    def start_recognition(self):
        """å¼€å§‹æ‰‹åŠ¿è¯†åˆ«"""
        if self.is_running:
            return "æ‰‹åŠ¿è¯†åˆ«å·²åœ¨è¿è¡Œä¸­", self.current_gesture, self.current_handedness
        
        # # æŸ¥æ‰¾æ‘„åƒå¤´
        # camera_id = self.find_external_camera()
        # if camera_id is None:
        #     return "é”™è¯¯: æœªæ£€æµ‹åˆ°å¯ç”¨æ‘„åƒå¤´", "æ— æ‰‹åŠ¿", "æœªæ£€æµ‹åˆ°æ‰‹"

        camera_id = self.rtsp_url
        
        # åˆå§‹åŒ–RTSPæµ - ä½¿ç”¨FFMPEGåç«¯ä¼˜åŒ–å»¶è¿Ÿ
        self.cap = cv2.VideoCapture(camera_id, cv2.CAP_FFMPEG)
        if not self.cap.isOpened():
            # å¦‚æœFFMPEGå¤±è´¥ï¼Œå°è¯•é»˜è®¤åç«¯
            print("FFMPEGåç«¯å¤±è´¥ï¼Œå°è¯•é»˜è®¤åç«¯...")
            self.cap = cv2.VideoCapture(camera_id)
            if not self.cap.isOpened():
                return f"é”™è¯¯: æ— æ³•æ‰“å¼€RTSPæµ {camera_id}", "æ— æ‰‹åŠ¿", "æœªæ£€æµ‹åˆ°æ‰‹"
        
        # RTSPæµä½å»¶è¿Ÿä¼˜åŒ–å‚æ•°
        self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)     # æœ€å°ç¼“å†²åŒºï¼Œå‡å°‘å»¶è¿Ÿ
        self.cap.set(cv2.CAP_PROP_FPS, 20)           # é™ä½å¸§ç‡ï¼Œå‡å°‘ç½‘ç»œè´Ÿæ‹…
        
        # è®¾ç½®è¶…æ—¶å‚æ•°ï¼Œé¿å…é˜»å¡
        self.cap.set(cv2.CAP_PROP_OPEN_TIMEOUT_MSEC, 3000)   # è¿æ¥è¶…æ—¶3ç§’
        self.cap.set(cv2.CAP_PROP_READ_TIMEOUT_MSEC, 1000)   # è¯»å–è¶…æ—¶1ç§’
        
        # å°è¯•è®¾ç½®ç¼–è§£ç å™¨ï¼ˆå¯é€‰ï¼‰
        try:
            self.cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('H', '2', '6', '4'))
        except:
            print("æ— æ³•è®¾ç½®H.264ç¼–è§£ç å™¨ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®")
        
        # åˆ›å»ºæ‰‹åŠ¿è¯†åˆ«å™¨
        try:
            self.recognizer = vision.GestureRecognizer.create_from_options(self.options)
            self.is_running = True
            
            # å¯åŠ¨æ‘„åƒå¤´çº¿ç¨‹
            self.camera_thread = threading.Thread(target=self.camera_loop)
            self.camera_thread.daemon = True
            self.camera_thread.start()
            
            return f"æ‰‹åŠ¿è¯†åˆ«å·²å¯åŠ¨ (æ‘„åƒå¤´ {camera_id})", self.current_gesture, self.current_handedness
            
        except Exception as e:
            self.cleanup()
            return f"å¯åŠ¨å¤±è´¥: {str(e)}", "æ— æ‰‹åŠ¿", "æœªæ£€æµ‹åˆ°æ‰‹"
    
    def stop_recognition(self):
        """åœæ­¢æ‰‹åŠ¿è¯†åˆ«"""
        if not self.is_running:
            return "æ‰‹åŠ¿è¯†åˆ«æœªåœ¨è¿è¡Œ", "æ— æ‰‹åŠ¿", "æœªæ£€æµ‹åˆ°æ‰‹"
        
        self.cleanup()
        return "æ‰‹åŠ¿è¯†åˆ«å·²åœæ­¢", "æ— æ‰‹åŠ¿", "æœªæ£€æµ‹åˆ°æ‰‹"
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.is_running = False
        
        if self.camera_thread and self.camera_thread.is_alive():
            self.camera_thread.join(timeout=1.0)
        
        if self.cap:
            self.cap.release()
            self.cap = None
        
        if self.recognizer:
            self.recognizer.close()
            self.recognizer = None
        
        # å…³é—­çº¿ç¨‹æ± 
        if hasattr(self, 'executor'):
            print("æ­£åœ¨å…³é—­æœºå™¨ç‹—æ§åˆ¶çº¿ç¨‹æ± ...")
            self.executor.shutdown(wait=True, timeout=3.0)
        
        self.current_frame = None
        self.latest_result = None
        self.current_gesture = "æ— æ‰‹åŠ¿"
        self.current_handedness = "æœªæ£€æµ‹åˆ°æ‰‹"
    
    def get_current_frame(self):
        """è·å–å½“å‰å¸§"""
        if self.current_frame is not None:
            # è½¬æ¢ä¸ºPIL Imageæ ¼å¼ä¾›Gradioæ˜¾ç¤º
            return Image.fromarray(self.current_frame)
        else:
            # è¿”å›é»‘è‰²å›¾åƒ
            black_image = np.zeros((360, 480, 3), dtype=np.uint8)
            return Image.fromarray(black_image)
    
    def get_status_info(self):
        """è·å–çŠ¶æ€ä¿¡æ¯"""
        return self.current_gesture, self.current_handedness, self.current_robot_action

# åˆ›å»ºå…¨å±€è¯†åˆ«å™¨å®ä¾‹
gesture_recognizer = GradioGestureRecognizer()

def start_recognition():
    """å¯åŠ¨è¯†åˆ«çš„åŒ…è£…å‡½æ•°"""
    status, gesture, handedness = gesture_recognizer.start_recognition()
    return status, gesture, handedness

def stop_recognition():
    """åœæ­¢è¯†åˆ«çš„åŒ…è£…å‡½æ•°"""
    status, gesture, handedness = gesture_recognizer.stop_recognition()
    return status, gesture, handedness

def toggle_robot_control():
    """åˆ‡æ¢æœºå™¨ç‹—æ§åˆ¶çŠ¶æ€çš„åŒ…è£…å‡½æ•°"""
    enabled = gesture_recognizer.toggle_robot_control()
    status = "æœºå™¨ç‹—æ§åˆ¶å·²å¯ç”¨" if enabled else "æœºå™¨ç‹—æ§åˆ¶å·²ç¦ç”¨"
    return status, gesture_recognizer.current_robot_action

def update_display():
    """æ›´æ–°æ˜¾ç¤ºçš„åŒ…è£…å‡½æ•°"""
    frame = gesture_recognizer.get_current_frame()
    gesture, handedness, robot_action = gesture_recognizer.get_status_info()
    return frame, gesture, handedness, robot_action

# åˆ›å»ºGradioç•Œé¢
def create_interface():
    with gr.Blocks(title="æ‰‹åŠ¿è¯†åˆ«", theme=gr.themes.Soft()) as interface:
        gr.Markdown(
            """
            # ğŸ¤– æ‰‹åŠ¿è¯†åˆ« + æœºå™¨ç‹—æ§åˆ¶ç³»ç»Ÿ
            
            **å¤šæ‰‹è¯†åˆ«åŠŸèƒ½**:
            - ğŸ–ï¸ å¯åŒæ—¶è¯†åˆ«æœ€å¤š5åªæ‰‹
            - ğŸ¯ è‡ªåŠ¨é€‰æ‹©æœ€å¤§çš„æ‰‹ï¼ˆæœ€é è¿‘æ‘„åƒå¤´ï¼‰ä½œä¸ºæ§åˆ¶æ‰‹
            - ğŸ”´ ä¸»æ§æ‰‹ç”¨çº¢è‰²é«˜äº®æ˜¾ç¤ºï¼Œå…¶ä»–æ‰‹ç”¨ç»¿è‰²æ˜¾ç¤º
            
            **æ”¯æŒçš„æ‰‹åŠ¿æ§åˆ¶**:
            - ğŸ‘† Pointing_Up â†’ å‰è¿›
            - ğŸ‘‡ Thumb_Down â†’ åé€€  
            - âœ‹ Open_Palm (å·¦æ‰‹) â†’ å·¦ç§»
            - âœ‹ Open_Palm (å³æ‰‹) â†’ å³ç§»
            - ğŸ‘Š Closed_Fist (å·¦æ‰‹) â†’ å·¦è½¬
            - ğŸ‘Š Closed_Fist (å³æ‰‹) â†’ å³è½¬
            - âœŒï¸ Victory â†’ åœæ­¢
            """
        )
        
        with gr.Row():
            # å·¦ä¾§ï¼šè§†é¢‘æ˜¾ç¤º
            with gr.Column(scale=2):
                video_output = gr.Image(
                    label="ğŸ“¹ æ‘„åƒå¤´è§†é¢‘æµ",
                    type="pil",
                    interactive=False,
                    height=600
                )
            
            # å³ä¾§ï¼šè¯†åˆ«ç»“æœå’Œæ§åˆ¶
            with gr.Column(scale=1):
                with gr.Group():
                    gr.Markdown("### ğŸ¯ è¯†åˆ«ç»“æœ")
                    
                    gesture_output = gr.Textbox(
                        label="æ£€æµ‹åˆ°çš„æ‰‹åŠ¿",
                        value="æ— æ‰‹åŠ¿",
                        interactive=False,
                        lines=2
                    )
                    
                    handedness_output = gr.Textbox(
                        label="å·¦æ‰‹/å³æ‰‹",
                        value="æœªæ£€æµ‹åˆ°æ‰‹",
                        interactive=False,
                        lines=2
                    )
                    
                    robot_action_output = gr.Textbox(
                        label="ğŸ• æœºå™¨ç‹—åŠ¨ä½œ",
                        value="åœæ­¢",
                        interactive=False,
                        lines=2
                    )
                
                with gr.Group():
                    gr.Markdown("### ğŸ® æ§åˆ¶é¢æ¿")
                    
                    start_btn = gr.Button(
                        "â–¶ï¸ å¼€å§‹è¯†åˆ«",
                        variant="primary",
                        size="lg"
                    )
                    
                    stop_btn = gr.Button(
                        "â¹ï¸ åœæ­¢è¯†åˆ«",
                        variant="secondary",
                        size="lg"
                    )
                    
                    robot_control_btn = gr.Button(
                        "ğŸ• å¯ç”¨/ç¦ç”¨æœºå™¨ç‹—æ§åˆ¶",
                        variant="secondary",
                        size="lg"
                    )
                    
                    status_output = gr.Textbox(
                        label="ç³»ç»ŸçŠ¶æ€",
                        value="å°±ç»ª",
                        interactive=False,
                        lines=2
                    )
                    
                    robot_status_output = gr.Textbox(
                        label="ğŸ• æœºå™¨ç‹—çŠ¶æ€",
                        value="æœºå™¨ç‹—æ§åˆ¶å·²ç¦ç”¨",
                        interactive=False,
                        lines=2
                    )
        
        # æŒ‰é’®äº‹ä»¶ç»‘å®š
        start_btn.click(
            fn=start_recognition,
            outputs=[status_output, gesture_output, handedness_output]
        )
        
        stop_btn.click(
            fn=stop_recognition,
            outputs=[status_output, gesture_output, handedness_output]
        )
        
        robot_control_btn.click(
            fn=toggle_robot_control,
            outputs=[robot_status_output, robot_action_output]
        )
        
        # ä½¿ç”¨æ›´é«˜é¢‘ç‡çš„å®šæ—¶å™¨æ›´æ–°æ˜¾ç¤º
        timer = gr.Timer(0.03)  # 50msæ›´æ–°ä¸€æ¬¡ï¼Œæé«˜æµç•…åº¦
        timer.tick(
            fn=update_display,
            outputs=[video_output, gesture_output, handedness_output, robot_action_output]
        )
    
    return interface

if __name__ == "__main__":
    # åˆ›å»ºå¹¶å¯åŠ¨ç•Œé¢
    interface = create_interface()
    
    try:
        # å¯åŠ¨Gradioåº”ç”¨
        interface.launch(
            server_name="127.0.0.1",  # åªå…è®¸æœ¬åœ°è®¿é—®
            server_port=7870,         # ç«¯å£å·            
            debug=False
        )
    except KeyboardInterrupt:
        print("\næ­£åœ¨å…³é—­åº”ç”¨...")
    finally:
        # æ¸…ç†èµ„æº
        gesture_recognizer.cleanup()
        print("åº”ç”¨å·²å…³é—­")
