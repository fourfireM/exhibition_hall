import cv2
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
import numpy as np
import time
import gradio as gr
import threading
from PIL import Image
import asyncio
import concurrent.futures
from move import (
    move_forward, move_backward, turn_left, turn_right,
    strafe_left, strafe_right, stop_movement
)
import os 
import requests
import ffmpeg

os.environ["OPENCV_FFMPEG_CAPTURE_OPTIONS"] = "rtsp_transport;tcp"

class GradioGestureRecognizer:
    def __init__(self):
        # é…ç½®æ¨¡å‹è·¯å¾„
        self.model_path = '/home/myb/handposedemodog/gesture_recognizer.task'
        
        # åˆå§‹åŒ–å˜é‡
        self.latest_result = None
        self.latest_timestamp = 0
        self.is_running = False
        self.ffmpeg_process = None
        self.recognizer = None
        self.camera_thread = None
        self.current_frame = None
        self.current_gesture = "æ— æ‰‹åŠ¿"
        self.current_handedness = "æœªæ£€æµ‹åˆ°æ‰‹"
        
        # è§†é¢‘æµå‚æ•°
        self.width = 640
        self.height = 480
        
        # æ€§èƒ½ä¼˜åŒ–ç›¸å…³
        self.frame_skip_counter = 0
        self.frame_skip_interval = 2  # æ¯1å¸§å¤„ç†ä¸€æ¬¡æ‰‹åŠ¿è¯†åˆ«ï¼Œæé«˜å“åº”é€Ÿåº¦
        self.last_process_time = time.time()
        
        # æœºå™¨ç‹—æ§åˆ¶ç›¸å…³
        self.robot_control_enabled = False
        self.current_robot_action = "åœæ­¢"
        self.last_gesture_time = 0
        self.gesture_cooldown = 0.08  # æ‰‹åŠ¿è¯†åˆ«å†·å´æ—¶é—´ï¼ˆç§’ï¼‰è¿›ä¸€æ­¥å‡å°‘
        self.min_confidence = 0.5  # æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼
        self.last_action_name = None  # è®°å½•ä¸Šä¸€ä¸ªåŠ¨ä½œï¼Œç”¨äºå¿«é€Ÿåˆ‡æ¢æ£€æµ‹
        
        # å¼‚æ­¥æ‰§è¡Œç›¸å…³
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)
        self.pending_actions = set()  # è·Ÿè¸ªæ­£åœ¨æ‰§è¡Œçš„åŠ¨ä½œ
        
        # RTSPæµç›¸å…³
        rtsp_data = requests.get("http://localhost:18080/signalservice/video/open")
        if os.environ.get("VIDEO_SOURCE") == "unitree":
            self.rtsp_url = rtsp_data.json()["data"]["unitree_rtsp_url"]
            # self.rtsp_url = self.rtsp_url + "?tcp"
        elif os.environ.get("VIDEO_SOURCE") == "realsense":
            self.rtsp_url = rtsp_data.json()["data"]["realsense_rtsp_url"]
        elif os.environ.get("VIDEO_SOURCE") == "orbbec":
            self.rtsp_url = rtsp_data.json()["data"]["orbbec_rtsp_url"]
        else:
            self.rtsp_url = rtsp_data.json()["data"]["lite3_rtsp_url"]
        self.rtsp_reconnect_attempts = 0
        self.max_reconnect_attempts = 3
        
        # åˆ›å»ºæ‰‹åŠ¿è¯†åˆ«å™¨é€‰é¡¹
        base_options = python.BaseOptions(model_asset_path=self.model_path)
        self.options = vision.GestureRecognizerOptions(
            base_options=base_options,
            running_mode=vision.RunningMode.LIVE_STREAM,
            result_callback=self.process_result,
            num_hands=1
        )
    
    def process_result(self, result, output_image, timestamp_ms):
        """å¤„ç†æ‰‹åŠ¿è¯†åˆ«ç»“æœçš„å›è°ƒå‡½æ•°"""
        self.latest_result = result
        self.latest_timestamp = timestamp_ms
        
        # æ›´æ–°æ‰‹åŠ¿ä¿¡æ¯
        gesture_name = None
        confidence = 0.0
        hand_label = None
        
        if result.gestures and result.gestures[0]:
            gesture_name = result.gestures[0][0].category_name
            confidence = result.gestures[0][0].score
            self.current_gesture = f"{gesture_name} ({confidence:.2f})"
        else:
            self.current_gesture = "æ— æ‰‹åŠ¿"
        
        # æ›´æ–°æ‰‹æ€§ä¿¡æ¯
        if result.handedness and result.handedness[0]:
            hand_label = result.handedness[0][0].category_name
            if hand_label == "Left":
                hand_label = "Right"  # ç¿»è½¬åçš„å·¦æ‰‹å®é™…æ˜¯å³æ‰‹
            elif hand_label == "Right": 
                hand_label = "Left"   # ç¿»è½¬åçš„å³æ‰‹å®é™…æ˜¯å·¦æ‰‹

            hand_score = result.handedness[0][0].score
            self.current_handedness = f"{hand_label} ({hand_score:.2f})"
        else:
            self.current_handedness = "æœªæ£€æµ‹åˆ°æ‰‹"
        
        # æœºå™¨ç‹—æ§åˆ¶é€»è¾‘
        if gesture_name and confidence > 0:
            robot_action = self.map_gesture_to_robot_action(gesture_name, hand_label, confidence)
            if robot_action:
                action_name, action_func = robot_action
                action_result = self.execute_robot_action(action_name, action_func)
                print(f"æœºå™¨ç‹—æ§åˆ¶: {action_result}")
        # æ²¡æœ‰æ£€æµ‹åˆ°æ‰‹åŠ¿æ—¶ä¸éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œå› ä¸ºå·²ç»ç§»é™¤äº†ç¨³å®šæ€§æ£€æŸ¥
    
    def map_gesture_to_robot_action(self, gesture_name, hand_label, confidence):
        """
        å°†æ‰‹åŠ¿æ˜ å°„åˆ°æœºå™¨ç‹—åŠ¨ä½œ
        
        æ˜ å°„è§„åˆ™ï¼š
        ğŸ‘† Pointing_Upï¼ˆå•æ‰‹å‘ä¸ŠæŒ‡ï¼‰    â†’ å‰è¿›
        ğŸ‘‡ Thumb_Downï¼ˆæ‹‡æŒ‡å‘ä¸‹ï¼‰      â†’ åé€€  
        âœ‹ Open_Palmï¼ˆå·¦æ‰‹å¼ å¼€ï¼‰       â†’ å·¦ç§»
        âœ‹ Open_Palmï¼ˆå³æ‰‹å¼ å¼€ï¼‰       â†’ å³ç§»
        ğŸ‘Š Closed_Fistï¼ˆå·¦æ‰‹æ¡æ‹³ï¼‰     â†’ å·¦è½¬
        ğŸ‘Š Closed_Fistï¼ˆå³æ‰‹æ¡æ‹³ï¼‰     â†’ å³è½¬
        ğŸ›‘ Victoryï¼ˆVæ‰‹åŠ¿ï¼‰           â†’ åœæ­¢
        """
        if not self.robot_control_enabled or confidence < self.min_confidence:
            return None
        
        current_time = time.time()
        
        # æ£€æŸ¥å†·å´æ—¶é—´
        if current_time - self.last_gesture_time < self.gesture_cooldown:
            return None
        
        # æ‰‹åŠ¿åˆ°åŠ¨ä½œçš„æ˜ å°„
        action_map = {
            ("Pointing_Up", None): ("å‰è¿›", move_forward),
            ("Thumb_Down", None): ("åé€€", move_backward),
            # ("Open_Palm", "Left"): ("å·¦ç§»", strafe_left),
            # ("Open_Palm", "Right"): ("å³ç§»", strafe_right),
            # ("Closed_Fist", "Left"): ("å·¦è½¬", turn_left),
            # ("Closed_Fist", "Right"): ("å³è½¬", turn_right),
            ("Open_Palm", "Left"): ("å·¦ç§»", strafe_right),
            ("Open_Palm", "Right"): ("å³ç§»", strafe_left),
            ("Closed_Fist", "Left"): ("å·¦è½¬", turn_right),
            ("Closed_Fist", "Right"): ("å³è½¬", turn_left),
            ("Victory", None): ("åœæ­¢", stop_movement),
        }
        
        # æŸ¥æ‰¾åŒ¹é…çš„åŠ¨ä½œ
        for (gesture, hand), (action_name, action_func) in action_map.items():
            if gesture_name == gesture and (hand is None or hand_label == hand):
                return action_name, action_func
        
        return None
    
    def execute_robot_action_async(self, action_name, action_func):
        """å¼‚æ­¥æ‰§è¡Œæœºå™¨ç‹—åŠ¨ä½œçš„åŒ…è£…å‡½æ•°"""
        try:
            print(f"å¼€å§‹å¼‚æ­¥æ‰§è¡Œ: {action_name}")
            result = action_func()
            print(f"å¼‚æ­¥æ‰§è¡Œå®Œæˆ: {action_name}")
            # ä»å¾…æ‰§è¡Œåˆ—è¡¨ä¸­ç§»é™¤
            self.pending_actions.discard(action_name)
            return result
        except Exception as e:
            print(f"å¼‚æ­¥æ‰§è¡Œå¤±è´¥: {action_name}, é”™è¯¯: {str(e)}")
            self.pending_actions.discard(action_name)
            return f"æ‰§è¡Œå¤±è´¥: {str(e)}"

    def execute_robot_action(self, action_name, action_func):
        """æ‰§è¡Œæœºå™¨ç‹—åŠ¨ä½œ - å¼‚æ­¥éé˜»å¡ç‰ˆæœ¬"""
        try:
            current_time = time.time()
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒåŠ¨ä½œåœ¨æ‰§è¡Œä¸­
            if action_name in self.pending_actions:
                return f"æ‰§è¡Œä¸­: {action_name}"
            
            # åªè¿›è¡ŒåŸºæœ¬çš„å†·å´æ£€æŸ¥ï¼Œé¿å…è¿‡äºé¢‘ç¹çš„ç›¸åŒåŠ¨ä½œ
            time_since_last = current_time - self.last_gesture_time
            
            # å¦‚æœæ˜¯ä¸åŒçš„åŠ¨ä½œï¼Œç«‹å³æ‰§è¡Œ
            # å¦‚æœæ˜¯ç›¸åŒçš„åŠ¨ä½œï¼Œæ£€æŸ¥å†·å´æ—¶é—´
            if self.last_action_name != action_name or time_since_last >= self.gesture_cooldown:
                # å¼‚æ­¥æ‰§è¡ŒåŠ¨ä½œï¼Œä¸é˜»å¡ä¸»çº¿ç¨‹
                self.pending_actions.add(action_name)
                future = self.executor.submit(self.execute_robot_action_async, action_name, action_func)
                
                # æ›´æ–°çŠ¶æ€ï¼ˆç«‹å³æ›´æ–°ï¼Œä¸ç­‰å¾…æ‰§è¡Œå®Œæˆï¼‰
                self.current_robot_action = action_name
                self.last_gesture_time = current_time
                self.last_action_name = action_name
                
                print(f"æäº¤æœºå™¨ç‹—åŠ¨ä½œ: {action_name} (å»¶è¿Ÿ: {time_since_last:.3f}s)")
                return f"æ‰§è¡Œ: {action_name}"
            else:
                # ç›¸åŒåŠ¨ä½œåœ¨å†·å´æœŸå†…
                return f"å†·å´ä¸­: {action_name} ({self.gesture_cooldown - time_since_last:.2f}s)"
            
        except Exception as e:
            print(f"æäº¤æœºå™¨ç‹—åŠ¨ä½œå¤±è´¥: {str(e)}")
            return f"æäº¤å¤±è´¥: {str(e)}"
    
    def toggle_robot_control(self):
        """åˆ‡æ¢æœºå™¨ç‹—æ§åˆ¶çŠ¶æ€"""
        self.robot_control_enabled = not self.robot_control_enabled
        if not self.robot_control_enabled:
            # åœç”¨æ§åˆ¶æ—¶è‡ªåŠ¨åœæ­¢æœºå™¨ç‹—
            try:
                stop_movement()
                self.current_robot_action = "åœæ­¢"
            except:
                pass
        return self.robot_control_enabled
    
    def reconnect_rtsp(self):
        """é‡è¿RTSPæµ"""
        if self.rtsp_reconnect_attempts >= self.max_reconnect_attempts:
            print(f"RTSPé‡è¿å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•° {self.max_reconnect_attempts}")
            return False
        
        self.rtsp_reconnect_attempts += 1
        print(f"å°è¯•é‡è¿RTSPæµ... (ç¬¬{self.rtsp_reconnect_attempts}æ¬¡)")
        
        # ç»ˆæ­¢å½“å‰ffmpegè¿›ç¨‹
        if self.ffmpeg_process:
            try:
                self.ffmpeg_process.terminate()
                self.ffmpeg_process.wait(timeout=3)
            except:
                pass
        
        # é‡æ–°è¿æ¥
        time.sleep(1)  # ç­‰å¾…1ç§’å†é‡è¿
        try:
            self.ffmpeg_process = (
                ffmpeg
                .input(self.rtsp_url, rtsp_transport='tcp', fflags="nobuffer", flags="low_delay", strict="experimental")
                .output('pipe:', format='rawvideo', pix_fmt='bgr24')
                .run_async(pipe_stdout=True)
            )
            
            print("RTSPæµé‡è¿æˆåŠŸ")
            self.rtsp_reconnect_attempts = 0  # é‡ç½®é‡è¿è®¡æ•°
            return True
        except Exception as e:
            print(f"RTSPæµé‡è¿å¤±è´¥: {e}")
            return False
    
    
    def draw_landmarks_on_image(self, rgb_image, detection_result):
        """åœ¨å›¾åƒä¸Šç»˜åˆ¶æ‰‹éƒ¨å…³é”®ç‚¹"""
        if not detection_result or not detection_result.hand_landmarks:
            return rgb_image
        
        annotated_image = np.copy(rgb_image)
        
        for hand_landmarks in detection_result.hand_landmarks:
            # å°†å…³é”®ç‚¹è½¬æ¢ä¸ºåƒç´ åæ ‡
            height, width, _ = annotated_image.shape
            landmark_points = []
            
            for landmark in hand_landmarks:
                x = int(landmark.x * width)
                y = int(landmark.y * height)
                landmark_points.append((x, y))
            
            # ç»˜åˆ¶å…³é”®ç‚¹
            for point in landmark_points:
                cv2.circle(annotated_image, point, 3, (0, 255, 0), -1)
            
            # ç»˜åˆ¶è¿æ¥çº¿ï¼ˆæ‰‹éƒ¨éª¨æ¶ï¼‰
            connections = [
                (0, 1), (1, 2), (2, 3), (3, 4),  # æ‹‡æŒ‡
                (0, 5), (5, 6), (6, 7), (7, 8),  # é£ŸæŒ‡
                (5, 9), (9, 10), (10, 11), (11, 12),  # ä¸­æŒ‡
                (9, 13), (13, 14), (14, 15), (15, 16),  # æ— åæŒ‡
                (13, 17), (17, 18), (18, 19), (19, 20),  # å°æŒ‡
                (0, 17)  # æ‰‹æŒ
            ]
            
            for connection in connections:
                if connection[0] < len(landmark_points) and connection[1] < len(landmark_points):
                    cv2.line(annotated_image, landmark_points[connection[0]], 
                            landmark_points[connection[1]], (255, 0, 0), 2)
        
        return annotated_image
    
    def camera_loop(self):
        """æ‘„åƒå¤´å¾ªç¯å¤„ç† - FFmpegä¼˜åŒ–ç‰ˆ"""
        frame_count = 0
        last_fps_time = time.time()
        
        while self.is_running and self.ffmpeg_process is not None:
            try:
                # ä»ffmpegè¿›ç¨‹è¯»å–åŸå§‹è§†é¢‘æ•°æ®
                in_bytes = self.ffmpeg_process.stdout.read(self.width * self.height * 3)
                if not in_bytes:
                    print("RTSPæµè¯»å–å¤±è´¥ï¼Œå°è¯•é‡è¿...")
                    if self.reconnect_rtsp():
                        continue  # é‡è¿æˆåŠŸï¼Œç»§ç»­å¾ªç¯
                    else:
                        break     # é‡è¿å¤±è´¥ï¼Œé€€å‡ºå¾ªç¯
                
                # å°†å­—èŠ‚æ•°æ®è½¬æ¢ä¸ºnumpyæ•°ç»„
                frame = np.frombuffer(in_bytes, np.uint8).reshape([self.height, self.width, 3])
                frame_count += 1
                
                # ç¿»è½¬å›¾åƒï¼Œä½¿å…¶åƒé•œå­ä¸€æ ·
                frame = cv2.flip(frame, 1)
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # å¸§è·³è¿‡ä¼˜åŒ–ï¼šä¸æ˜¯æ¯å¸§éƒ½è¿›è¡Œæ‰‹åŠ¿è¯†åˆ«
                self.frame_skip_counter += 1
                should_process = self.frame_skip_counter >= self.frame_skip_interval
                
                if should_process and self.recognizer:
                    # åˆ›å»ºMediaPipe Imageå¯¹è±¡
                    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)
                    
                    # è®¡ç®—æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
                    timestamp_ms = int(time.time() * 1000)
                    
                    # å¼‚æ­¥è¯†åˆ«æ‰‹åŠ¿
                    self.recognizer.recognize_async(mp_image, timestamp_ms)
                    self.frame_skip_counter = 0
                
                # ç»˜åˆ¶æ‰‹éƒ¨å…³é”®ç‚¹ï¼ˆæ¯å¸§éƒ½ç»˜åˆ¶ï¼Œä¿æŒè§†è§‰è¿ç»­æ€§ï¼‰
                if self.latest_result:
                    annotated_frame = self.draw_landmarks_on_image(rgb_frame, self.latest_result)
                    self.current_frame = annotated_frame
                else:
                    self.current_frame = rgb_frame
                
                # åŠ¨æ€FPSç›‘æ§
                if frame_count % 30 == 0:
                    current_time = time.time()
                    fps = 30 / (current_time - last_fps_time)
                    print(f"RTSPæµFPS: {fps:.1f}")
                    last_fps_time = current_time
                
                # å‡å°‘ç¡çœ æ—¶é—´ï¼Œæé«˜å“åº”é€Ÿåº¦
                time.sleep(0.01)  # 100fpså¤„ç†é€Ÿåº¦ï¼Œä½†å®é™…å—RTSPæµé™åˆ¶
                
            except Exception as e:
                print(f"è¯»å–è§†é¢‘æµå‡ºé”™: {e}")
                if self.reconnect_rtsp():
                    continue
                else:
                    break
    
    def start_recognition(self):
        """å¼€å§‹æ‰‹åŠ¿è¯†åˆ«"""
        if self.is_running:
            return "æ‰‹åŠ¿è¯†åˆ«å·²åœ¨è¿è¡Œä¸­", self.current_gesture, self.current_handedness
        
        rtsp_url = self.rtsp_url
        
        # åˆå§‹åŒ–RTSPæµ - ä½¿ç”¨FFmpegä¼˜åŒ–å»¶è¿Ÿ
        try:
            self.ffmpeg_process = (
                ffmpeg
                .input(rtsp_url, rtsp_transport='tcp', fflags="nobuffer", flags="low_delay", strict="experimental")
                .output('pipe:', format='rawvideo', pix_fmt='bgr24')
                .run_async(pipe_stdout=True)
            )
            print(f"FFmpegè¿›ç¨‹å·²å¯åŠ¨ï¼Œè¿æ¥åˆ°: {rtsp_url}")
        except Exception as e:
            return f"é”™è¯¯: æ— æ³•å¯åŠ¨FFmpegè¿›ç¨‹ {rtsp_url}, é”™è¯¯: {str(e)}", "æ— æ‰‹åŠ¿", "æœªæ£€æµ‹åˆ°æ‰‹"
        
        # åˆ›å»ºæ‰‹åŠ¿è¯†åˆ«å™¨
        try:
            self.recognizer = vision.GestureRecognizer.create_from_options(self.options)
            self.is_running = True
            
            # å¯åŠ¨æ‘„åƒå¤´çº¿ç¨‹
            self.camera_thread = threading.Thread(target=self.camera_loop)
            self.camera_thread.daemon = True
            self.camera_thread.start()
            
            return f"æ‰‹åŠ¿è¯†åˆ«å·²å¯åŠ¨ (RTSPæµ {rtsp_url})", self.current_gesture, self.current_handedness
            
        except Exception as e:
            self.cleanup()
            return f"å¯åŠ¨å¤±è´¥: {str(e)}", "æ— æ‰‹åŠ¿", "æœªæ£€æµ‹åˆ°æ‰‹"
    
    def stop_recognition(self):
        """åœæ­¢æ‰‹åŠ¿è¯†åˆ«"""
        if not self.is_running:
            return "æ‰‹åŠ¿è¯†åˆ«æœªåœ¨è¿è¡Œ", "æ— æ‰‹åŠ¿", "æœªæ£€æµ‹åˆ°æ‰‹"
        
        self.cleanup()
        return "æ‰‹åŠ¿è¯†åˆ«å·²åœæ­¢", "æ— æ‰‹åŠ¿", "æœªæ£€æµ‹åˆ°æ‰‹"
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.is_running = False
        
        if self.camera_thread and self.camera_thread.is_alive():
            self.camera_thread.join(timeout=1.0)
        
        if self.ffmpeg_process:
            try:
                self.ffmpeg_process.terminate()
                self.ffmpeg_process.wait(timeout=3)
                print("FFmpegè¿›ç¨‹å·²ç»ˆæ­¢")
            except Exception as e:
                print(f"ç»ˆæ­¢FFmpegè¿›ç¨‹æ—¶å‡ºé”™: {e}")
            self.ffmpeg_process = None
        
        if self.recognizer:
            self.recognizer.close()
            self.recognizer = None
        
        # å…³é—­çº¿ç¨‹æ± 
        if hasattr(self, 'executor'):
            print("æ­£åœ¨å…³é—­æœºå™¨ç‹—æ§åˆ¶çº¿ç¨‹æ± ...")
            self.executor.shutdown(wait=True, timeout=3.0)
        
        self.current_frame = None
        self.latest_result = None
        self.current_gesture = "æ— æ‰‹åŠ¿"
        self.current_handedness = "æœªæ£€æµ‹åˆ°æ‰‹"
    
    def get_current_frame(self):
        """è·å–å½“å‰å¸§"""
        if self.current_frame is not None:
            # è½¬æ¢ä¸ºPIL Imageæ ¼å¼ä¾›Gradioæ˜¾ç¤º
            return Image.fromarray(self.current_frame)
        else:
            # è¿”å›é»‘è‰²å›¾åƒ
            black_image = np.zeros((360, 480, 3), dtype=np.uint8)
            return Image.fromarray(black_image)
    
    def get_status_info(self):
        """è·å–çŠ¶æ€ä¿¡æ¯"""
        return self.current_gesture, self.current_handedness, self.current_robot_action

# åˆ›å»ºå…¨å±€è¯†åˆ«å™¨å®ä¾‹
gesture_recognizer = GradioGestureRecognizer()

def start_recognition():
    """å¯åŠ¨è¯†åˆ«çš„åŒ…è£…å‡½æ•°"""
    status, gesture, handedness = gesture_recognizer.start_recognition()
    return status, gesture, handedness

def stop_recognition():
    """åœæ­¢è¯†åˆ«çš„åŒ…è£…å‡½æ•°"""
    status, gesture, handedness = gesture_recognizer.stop_recognition()
    return status, gesture, handedness

def toggle_robot_control():
    """åˆ‡æ¢æœºå™¨ç‹—æ§åˆ¶çŠ¶æ€çš„åŒ…è£…å‡½æ•°"""
    enabled = gesture_recognizer.toggle_robot_control()
    status = "æœºå™¨ç‹—æ§åˆ¶å·²å¯ç”¨" if enabled else "æœºå™¨ç‹—æ§åˆ¶å·²ç¦ç”¨"
    return status, gesture_recognizer.current_robot_action

def update_display():
    """æ›´æ–°æ˜¾ç¤ºçš„åŒ…è£…å‡½æ•°"""
    frame = gesture_recognizer.get_current_frame()
    gesture, handedness, robot_action = gesture_recognizer.get_status_info()
    return frame, gesture, handedness, robot_action

# åˆ›å»ºGradioç•Œé¢
def create_interface():
    with gr.Blocks(title="æ‰‹åŠ¿è¯†åˆ«", theme=gr.themes.Soft()) as interface:
        gr.Markdown(
            """
            # ğŸ¤– æ‰‹åŠ¿è¯†åˆ« + æœºå™¨ç‹—æ§åˆ¶ç³»ç»Ÿ (FFmpegç‰ˆæœ¬)
            
            ä½¿ç”¨FFmpegä¼˜åŒ–çš„ä½å»¶è¿ŸRTSPè§†é¢‘æµå¤„ç†
            
            **æ”¯æŒçš„æ‰‹åŠ¿æ§åˆ¶**:
            - ğŸ‘† Pointing_Up â†’ å‰è¿›
            - ğŸ‘‡ Thumb_Down â†’ åé€€  
            - âœ‹ Open_Palm (å·¦æ‰‹) â†’ å·¦ç§»
            - âœ‹ Open_Palm (å³æ‰‹) â†’ å³ç§»
            - ğŸ‘Š Closed_Fist (å·¦æ‰‹) â†’ å·¦è½¬
            - ğŸ‘Š Closed_Fist (å³æ‰‹) â†’ å³è½¬
            - âœŒï¸ Victory â†’ åœæ­¢
            """
        )
        
        with gr.Row():
            # å·¦ä¾§ï¼šè§†é¢‘æ˜¾ç¤º
            with gr.Column(scale=2):
                video_output = gr.Image(
                    label="ğŸ“¹ æ‘„åƒå¤´è§†é¢‘æµ",
                    type="pil",
                    interactive=False,
                    height=600
                )
            
            # å³ä¾§ï¼šè¯†åˆ«ç»“æœå’Œæ§åˆ¶
            with gr.Column(scale=1):
                with gr.Group():
                    gr.Markdown("### ğŸ¯ è¯†åˆ«ç»“æœ")
                    
                    gesture_output = gr.Textbox(
                        label="æ£€æµ‹åˆ°çš„æ‰‹åŠ¿",
                        value="æ— æ‰‹åŠ¿",
                        interactive=False,
                        lines=2
                    )
                    
                    handedness_output = gr.Textbox(
                        label="å·¦æ‰‹/å³æ‰‹",
                        value="æœªæ£€æµ‹åˆ°æ‰‹",
                        interactive=False,
                        lines=2
                    )
                    
                    robot_action_output = gr.Textbox(
                        label="ğŸ• æœºå™¨ç‹—åŠ¨ä½œ",
                        value="åœæ­¢",
                        interactive=False,
                        lines=2
                    )
                
                with gr.Group():
                    gr.Markdown("### ğŸ® æ§åˆ¶é¢æ¿")
                    
                    start_btn = gr.Button(
                        "â–¶ï¸ å¼€å§‹è¯†åˆ«",
                        variant="primary",
                        size="lg"
                    )
                    
                    stop_btn = gr.Button(
                        "â¹ï¸ åœæ­¢è¯†åˆ«",
                        variant="secondary",
                        size="lg"
                    )
                    
                    robot_control_btn = gr.Button(
                        "ğŸ• å¯ç”¨/ç¦ç”¨æœºå™¨ç‹—æ§åˆ¶",
                        variant="secondary",
                        size="lg"
                    )
                    
                    status_output = gr.Textbox(
                        label="ç³»ç»ŸçŠ¶æ€",
                        value="å°±ç»ª",
                        interactive=False,
                        lines=2
                    )
                    
                    robot_status_output = gr.Textbox(
                        label="ğŸ• æœºå™¨ç‹—çŠ¶æ€",
                        value="æœºå™¨ç‹—æ§åˆ¶å·²ç¦ç”¨",
                        interactive=False,
                        lines=2
                    )
        
        # æŒ‰é’®äº‹ä»¶ç»‘å®š
        start_btn.click(
            fn=start_recognition,
            outputs=[status_output, gesture_output, handedness_output]
        )
        
        stop_btn.click(
            fn=stop_recognition,
            outputs=[status_output, gesture_output, handedness_output]
        )
        
        robot_control_btn.click(
            fn=toggle_robot_control,
            outputs=[robot_status_output, robot_action_output]
        )
        
        # ä½¿ç”¨æ›´é«˜é¢‘ç‡çš„å®šæ—¶å™¨æ›´æ–°æ˜¾ç¤º
        timer = gr.Timer(0.03)  # 50msæ›´æ–°ä¸€æ¬¡ï¼Œæé«˜æµç•…åº¦
        timer.tick(
            fn=update_display,
            outputs=[video_output, gesture_output, handedness_output, robot_action_output]
        )
    
    return interface

if __name__ == "__main__":
    # åˆ›å»ºå¹¶å¯åŠ¨ç•Œé¢
    interface = create_interface()
    
    try:
        # å¯åŠ¨Gradioåº”ç”¨
        interface.launch(
            server_name="127.0.0.1",  # åªå…è®¸æœ¬åœ°è®¿é—®
            server_port=7870,         # ç«¯å£å·            
            debug=False
        )
    except KeyboardInterrupt:
        print("\næ­£åœ¨å…³é—­åº”ç”¨...")
    finally:
        # æ¸…ç†èµ„æº
        gesture_recognizer.cleanup()
        print("åº”ç”¨å·²å…³é—­")
